{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This is the solution of the problem described in https://www.kaggle.com/competitions/amp-parkinsons-disease-progression-prediction/discussion/411398","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-17T19:46:54.620614Z","iopub.execute_input":"2023-05-17T19:46:54.620954Z","iopub.status.idle":"2023-05-17T19:46:54.670086Z","shell.execute_reply.started":"2023-05-17T19:46:54.620925Z","shell.execute_reply":"2023-05-17T19:46:54.669370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Part I: Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom tensorflow.keras.layers import Dropout\nfrom sklearn.preprocessing import RobustScaler\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras import regularizers\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport warnings\nfrom pathlib import Path\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import KNNImputer\nfrom keras.layers import Input, Dense, concatenate\nfrom keras.models import Model\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nfrom keras.losses import mean_absolute_percentage_error\nimport tensorflow as tf\nwarnings.filterwarnings('ignore')  \n%matplotlib inline\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:51:01.527133Z","iopub.execute_input":"2023-05-17T19:51:01.527473Z","iopub.status.idle":"2023-05-17T19:51:01.538849Z","shell.execute_reply.started":"2023-05-17T19:51:01.527447Z","shell.execute_reply":"2023-05-17T19:51:01.537657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_directory = Path('/kaggle/input/amp-parkinsons-disease-progression-prediction')\ntrain_clinical_df = pd.read_csv(dataset_directory / 'train_clinical_data.csv') \ntrain_peptide_df = pd.read_csv(dataset_directory / 'train_peptides.csv')\ntrain_protein_df = pd.read_csv(dataset_directory / 'train_proteins.csv') \nsupp_df = pd.read_csv(dataset_directory / 'supplemental_clinical_data.csv')\nsupp_df.dropna(subset=['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4','upd23b_clinical_state_on_medication'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:47:07.742543Z","iopub.execute_input":"2023-05-17T19:47:07.742970Z","iopub.status.idle":"2023-05-17T19:47:08.328155Z","shell.execute_reply.started":"2023-05-17T19:47:07.742943Z","shell.execute_reply":"2023-05-17T19:47:08.327495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Analysis of the train_clinical_df\n# # for each columns in the train_clinical_df, here are the counts of NaN values.\nclinical_df = train_clinical_df.copy()\nnan_updrs_1 = clinical_df['updrs_1'].isna().sum() \nnan_updrs_2 = clinical_df['updrs_2'].isna().sum()  \nnan_updrs_3 = clinical_df['updrs_3'].isna().sum()   \nnan_updrs_4 = clinical_df['updrs_4'].isna().sum()  \nnan_medical = clinical_df['upd23b_clinical_state_on_medication'].isna().sum()   \nprint(\"Train_Clinical_df: \\n(Rows, Columns):\", clinical_df.shape,\"\\nNumber of NAN in updrs_1, updrs_2, updrs_3, updrs_4,medication: \\n\", nan_updrs_1,nan_updrs_2, nan_updrs_3, nan_updrs_4, nan_medical)\nclinical_df = supp_df.copy()\nnan_updrs_1 = clinical_df['updrs_1'].isna().sum() \nnan_updrs_2 = clinical_df['updrs_2'].isna().sum()  \nnan_updrs_3 = clinical_df['updrs_3'].isna().sum()   \nnan_updrs_4 = clinical_df['updrs_4'].isna().sum()  \nnan_medical = clinical_df['upd23b_clinical_state_on_medication'].isna().sum()   \nprint(\"\\nSupplementary_Clinical_df: \\n(Rows, Columns):\", clinical_df.shape,\"\\nNumber of NAN in updrs_1, updrs_2, updrs_3, updrs_4,medication: \\n\", nan_updrs_1,nan_updrs_2, nan_updrs_3, nan_updrs_4, nan_medical)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:47:11.758642Z","iopub.execute_input":"2023-05-17T19:47:11.758984Z","iopub.status.idle":"2023-05-17T19:47:11.773449Z","shell.execute_reply.started":"2023-05-17T19:47:11.758955Z","shell.execute_reply":"2023-05-17T19:47:11.772363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. As shown in above, there are 1038/2615 rows in Train_clinical_df have missing values. In order to make use of all the information given in the train_clinical_df, we will use KNN imputation to fill the missing values. \n2. We merge the complete records (no missing values) from Supplementary_clinical_df to Train_clinical_df to augment the clinical dataset aiming to improve the performance of KNN imputation. \n3. Apply x = log (x+1) to transform the data into normal distribution like data. This can also improve the KNN imputation performance. \n","metadata":{}},{"cell_type":"markdown","source":"## Data cleaning ","metadata":{}},{"cell_type":"markdown","source":"1. remove rows with missing values in Supp_df in the specified columns (updrs_1, updrs_2, updrs_3, updrs_4', 'upd23b_clinical_state_on_medication') to keep only the accurate records to augment the train_clinical_df. \n","metadata":{}},{"cell_type":"markdown","source":"## To analyze the trend in the visit_month column of the DataFrame, we plot a line graph of the average UPDRS scores (i.e., updrs_1, updrs_2, updrs_3, and updrs_4) against the visit_month.\n\n* We can compare the trends of 4 figures using the cleaned dataset, the cleaned dataset filtered by medication \"ON\", and the cleaned dataset filtered by medication \"OFF\".","metadata":{}},{"cell_type":"code","source":"# create three subplots in rows\nfig, ax = plt.subplots(ncols = 4,  figsize=(10, 6))\n\n############## the status of \"upd23b_clinical_state_on_medication\"] is ignored \n# plot the first figure\n# read the DataFrame from a file\ndf = train_clinical_df\n# group the DataFrame by visit_month and calculate the average of UPDRS scores for each group\ndf_avg = df.groupby('visit_month').mean()\n# plot the line graph of the average UPDRS scores against visit_month\nax[0].plot(df_avg.index, df_avg['updrs_1'], label='UPDRS-1')\nax[0].plot(df_avg.index, df_avg['updrs_2'], label='UPDRS-2')\nax[0].plot(df_avg.index, df_avg['updrs_3'], label='UPDRS-3')\nax[0].plot(df_avg.index, df_avg['updrs_4'], label='UPDRS-4')\nax[0].set_xlabel('Visit Month')\nax[0].set_ylabel('Average UPDRS Score')\nax[0].legend()\nax[0].set_title('Medication = \"OFF\" or \"ON\", \\n # of samples: {}'.format(df.shape[0]))\n############## the status of \"upd23b_clinical_state_on_medication\"] is NaN \n# concatenate the two DataFrames vertically\ndf = pd.concat([train_clinical_df, supp_df], ignore_index=True)\n# Filter df to select only the rows where upd23b_clinical_state_on_medication is 'On'\nnan_medication_df = df[df[\"upd23b_clinical_state_on_medication\"].isna()]\n# group the DataFrame by visit_month and calculate the average of UPDRS scores for each group\ndf_avg = nan_medication_df.iloc[:, :-1].groupby('visit_month').mean()\n# group the DataFrame by visit_month and calculate the average of UPDRS scores for each group\ndf_avg = df.groupby('visit_month').mean()\n# plot the line graph of the average UPDRS scores against visit_month\nax[3].plot(df_avg.index, df_avg['updrs_1'], label='UPDRS-1')\nax[3].plot(df_avg.index, df_avg['updrs_2'], label='UPDRS-2')\nax[3].plot(df_avg.index, df_avg['updrs_3'], label='UPDRS-3')\nax[3].plot(df_avg.index, df_avg['updrs_4'], label='UPDRS-4')\nax[3].set_xlabel('Visit Month')\nax[3].set_ylabel('Average UPDRS Score')\nax[3].legend()\nax[3].set_title('Medication = NaN, \\n # of samples: {}'.format(nan_medication_df.shape[0]))\n\n############## \"upd23b_clinical_state_on_medication\"] == \"On\"\n# Filter df to select only the rows where upd23b_clinical_state_on_medication is 'On'\non_medication_df = df[df[\"upd23b_clinical_state_on_medication\"] == \"On\"]\n# group the DataFrame by visit_month and calculate the average of UPDRS scores for each group\ndf_avg = on_medication_df.iloc[:, :-1].groupby('visit_month').mean()\n\n# plot the line graph of the average UPDRS scores against visit_month\nax[1].plot(df_avg.index, df_avg['updrs_1'], label='UPDRS-1')\nax[1].plot(df_avg.index, df_avg['updrs_2'], label='UPDRS-2')\nax[1].plot(df_avg.index, df_avg['updrs_3'], label='UPDRS-3')\nax[1].plot(df_avg.index, df_avg['updrs_4'], label='UPDRS-4')\nax[1].set_xlabel('Visit Month')\nax[1].legend()\nax[1].set_ylabel('Average UPDRS Score')\nax[1].set_title('Medication = \"ON\",\\n # of samples: {}'.format(on_medication_df.shape[0]))\n\n\n############## \"upd23b_clinical_state_on_medication\"] == \"Off\"\n# concatenate the two DataFrames vertically\ndf = pd.concat([train_clinical_df, supp_df], ignore_index=True)\n# Filter df to select only the rows where upd23b_clinical_state_on_medication is 'On'\noff_medication_df = df[df[\"upd23b_clinical_state_on_medication\"] == \"Off\"]\n# group the DataFrame by visit_month and calculate the average of UPDRS scores for each group\ndf_avg = off_medication_df.iloc[:, :-1].groupby('visit_month').mean()\n\n# plot the line graph of the average UPDRS scores against visit_month\nax[2].plot(df_avg.index, df_avg['updrs_1'], label='UPDRS-1')\nax[2].plot(df_avg.index, df_avg['updrs_2'], label='UPDRS-2')\nax[2].plot(df_avg.index, df_avg['updrs_3'], label='UPDRS-3')\nax[2].plot(df_avg.index, df_avg['updrs_4'], label='UPDRS-4')\nax[2].set_xlabel('Visit Month')\nax[2].set_ylabel('Average UPDRS Score')\nax[2].legend()\nax[2].set_title('Medication = \"OFF\",\\n # of samples: {}'.format(off_medication_df.shape[0]))\n\n# adjust the subplots layout\nplt.tight_layout()\n\n# show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:47:18.972934Z","iopub.execute_input":"2023-05-17T19:47:18.973275Z","iopub.status.idle":"2023-05-17T19:47:19.772122Z","shell.execute_reply.started":"2023-05-17T19:47:18.973246Z","shell.execute_reply":"2023-05-17T19:47:19.771015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate the two train_clinical_df and supp_df vertically\ntrain_df = pd.concat([train_clinical_df, supp_df], ignore_index=True)\n\n# assume df is your DataFrame\ncols_to_normalize = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']\n# Apply log(1+x) transformation to the last 4 columns\ntrain_df[cols_to_normalize] = np.log1p(train_df[cols_to_normalize])\n\n# perform KNN imputation on the normalized data\nimputer = KNNImputer(n_neighbors=10)\n\non_medication_mask = train_df[\"upd23b_clinical_state_on_medication\"] == \"On\"\ntrain_df.loc[on_medication_mask, cols_to_normalize] = np.maximum(imputer.fit_transform(train_df.loc[on_medication_mask, cols_to_normalize]), 0)\noff_medication_mask = train_df[\"upd23b_clinical_state_on_medication\"] == \"Off\"\ntrain_df.loc[off_medication_mask, cols_to_normalize] = np.maximum(imputer.fit_transform(train_df.loc[off_medication_mask, cols_to_normalize]), 0)\nnan_medication_mask = train_df[\"upd23b_clinical_state_on_medication\"].isna()\ntrain_df.loc[nan_medication_mask, cols_to_normalize] = np.maximum(imputer.fit_transform(train_df.loc[nan_medication_mask, cols_to_normalize]), 0)\n\ntrain_df[cols_to_normalize]=round(np.maximum(np.expm1(train_df[cols_to_normalize]),0))\ntrain_df = train_df.head(n= 2615)\ntrain_df = train_df.iloc[:, :-1]\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:47:26.483015Z","iopub.execute_input":"2023-05-17T19:47:26.484070Z","iopub.status.idle":"2023-05-17T19:47:26.664278Z","shell.execute_reply.started":"2023-05-17T19:47:26.484035Z","shell.execute_reply":"2023-05-17T19:47:26.662371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyze the frequence of visit_month ","metadata":{}},{"cell_type":"code","source":"# create a new column with a unique identifier for each patient\ndf = train_df.copy(deep=True)\ndf['patient_index'] = df.groupby('patient_id').ngroup()\n\n# create a scatter plot for patient_index versus visit_month\nplt.scatter(df['patient_index'], df['visit_month'], s=1)\n\n#plt.scatter(df_new_index.index, df_new_index[\"visit_month\"], s=1)\nplt.xlabel(\"Patient ID\")\nplt.ylabel(\"Visit Month\")\nplt.title(\"Visit Months for Each Patient\")\n\n# display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:47:32.808359Z","iopub.execute_input":"2023-05-17T19:47:32.808972Z","iopub.status.idle":"2023-05-17T19:47:32.940636Z","shell.execute_reply.started":"2023-05-17T19:47:32.808939Z","shell.execute_reply":"2023-05-17T19:47:32.939629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In the figure above, the interval between visit_month is approximately 6 months until the 60th month, after which the data becomes sparse. This suggests that only a few patient_id have long-term records beyond 60 months. Therefore, we will focus only on the data from the 60 months prior to the current time period. \n* Since the target of the prediction is from the current time point to 6, 6, 12 months, it would be helpful to include data up to 72 months to gather as much information as possible in our training dataset.","metadata":{}},{"cell_type":"code","source":"# Part II. Construct time series data","metadata":{"execution":{"iopub.status.busy":"2023-05-17T12:50:23.984961Z","iopub.execute_input":"2023-05-17T12:50:23.985355Z","iopub.status.idle":"2023-05-17T12:50:23.991480Z","shell.execute_reply.started":"2023-05-17T12:50:23.985316Z","shell.execute_reply":"2023-05-17T12:50:23.990244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.rename(columns={\n    'updrs_1': 'updrs_1_plus_0_months',\n    'updrs_2': 'updrs_2_plus_0_months',\n    'updrs_3': 'updrs_3_plus_0_months',\n    'updrs_4': 'updrs_4_plus_0_months'\n})\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:47:38.248453Z","iopub.execute_input":"2023-05-17T19:47:38.248839Z","iopub.status.idle":"2023-05-17T19:47:38.268613Z","shell.execute_reply.started":"2023-05-17T19:47:38.248803Z","shell.execute_reply":"2023-05-17T19:47:38.267504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define an empty DataFrame with column names\nts_df_updrs = pd.DataFrame(columns=['time', 'patient_id',  'visit_id',\n                                    'updrs_1_plus_0_months','updrs_2_plus_0_months','updrs_3_plus_0_months', 'updrs_4_plus_0_months',\n                                    'updrs_1_plus_6_months','updrs_2_plus_6_months','updrs_3_plus_6_months', 'updrs_4_plus_6_months',\n                                    'updrs_1_plus_12_months','updrs_2_plus_12_months','updrs_3_plus_12_months', 'updrs_4_plus_12_months',\n                                    'updrs_1_plus_24_months','updrs_2_plus_24_months','updrs_3_plus_24_months', 'updrs_4_plus_24_months'\n                                    ])\n\nfor patient_id in train_df['patient_id'].unique():\n    # filter a subset of visiting records for a selected patient_id \n    p_df = train_df[train_df['patient_id']==patient_id]\n    ts_start = p_df.visit_month.min()\n    ts_end = p_df.visit_month.max()\n    ts_span = ts_end - ts_start\n    p_num = p_df.shape[0]\n    if (p_num > 1) and (ts_span >= 30) and (ts_span/(p_df.shape[0]-1) < 24):\n        # the last visit record is 72\n        if ts_end > 72:\n            ts_end = 72+1\n\n        patient_df = pd.DataFrame({'time': range(ts_start, ts_end, 6), 'patient_id': patient_id})\n        # Add the visit_id column\n        patient_df['visit_id'] = patient_df['patient_id'].astype(str) + '_' + patient_df['time'].astype(str)\n        # merge dataframes on visit_id using left join\n        merged_df = pd.merge(patient_df, p_df, on='visit_id', how='left')\n        merged_df.drop('patient_id_y', axis=1,inplace = True)\n        merged_df.rename(columns={'patient_id_x': 'patient_id'},inplace = True)\n        # forward fill missing values\n        merged_df = merged_df.fillna(method='ffill')\n        # generate new columns updrs_p6_1, updrs_p6_2, updrs_p6_3, updrs_p6_4\n        merged_df['updrs_1_plus_6_months'] = merged_df['updrs_1_plus_0_months'].shift(-1)\n        merged_df['updrs_2_plus_6_months'] = merged_df['updrs_2_plus_0_months'].shift(-1)\n        merged_df['updrs_3_plus_6_months'] = merged_df['updrs_3_plus_0_months'].shift(-1)\n        merged_df['updrs_4_plus_6_months'] = merged_df['updrs_4_plus_0_months'].shift(-1)\n        merged_df['updrs_1_plus_12_months'] = merged_df['updrs_1_plus_0_months'].shift(-2)\n        merged_df['updrs_2_plus_12_months'] = merged_df['updrs_2_plus_0_months'].shift(-2)\n        merged_df['updrs_3_plus_12_months'] = merged_df['updrs_3_plus_0_months'].shift(-2)\n        merged_df['updrs_4_plus_12_months'] = merged_df['updrs_4_plus_0_months'].shift(-2)\n        merged_df['updrs_1_plus_24_months'] = merged_df['updrs_1_plus_0_months'].shift(-4)\n        merged_df['updrs_2_plus_24_months'] = merged_df['updrs_2_plus_0_months'].shift(-4)\n        merged_df['updrs_3_plus_24_months'] = merged_df['updrs_3_plus_0_months'].shift(-4)\n        merged_df['updrs_4_plus_24_months'] = merged_df['updrs_4_plus_0_months'].shift(-4)\n    \n        merged_df = merged_df.drop('visit_month', axis=1)\n        merged_df = merged_df.dropna()\n        ts_df_updrs= pd.concat([ts_df_updrs, merged_df],ignore_index=True)\n        \n        \n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:47:41.845842Z","iopub.execute_input":"2023-05-17T19:47:41.847156Z","iopub.status.idle":"2023-05-17T19:47:44.740269Z","shell.execute_reply.started":"2023-05-17T19:47:41.847052Z","shell.execute_reply":"2023-05-17T19:47:44.739212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ts_df_updrs","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:47:47.908143Z","iopub.execute_input":"2023-05-17T19:47:47.908582Z","iopub.status.idle":"2023-05-17T19:47:47.943120Z","shell.execute_reply.started":"2023-05-17T19:47:47.908544Z","shell.execute_reply":"2023-05-17T19:47:47.941901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part II: Extract features from Proteins and Peptides","metadata":{}},{"cell_type":"markdown","source":"## Join Protein and Peptide on 'visit_id' and 'UniProt'","metadata":{}},{"cell_type":"code","source":"Prot_Peptides_df = pd.merge(train_protein_df, train_peptide_df, on = ['visit_id','UniProt'], how = 'left')\nProt_Peptides_df = Prot_Peptides_df[['visit_id','UniProt','NPX','Peptide','PeptideAbundance']]\nProt_Peptides_df","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:47:52.479118Z","iopub.execute_input":"2023-05-17T19:47:52.479506Z","iopub.status.idle":"2023-05-17T19:47:52.904484Z","shell.execute_reply.started":"2023-05-17T19:47:52.479470Z","shell.execute_reply":"2023-05-17T19:47:52.903373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* check are there NaN values in Prot_Peptides_df","metadata":{}},{"cell_type":"code","source":"Prot_Peptides_df.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T12:50:28.832407Z","iopub.execute_input":"2023-05-17T12:50:28.832800Z","iopub.status.idle":"2023-05-17T12:50:29.180885Z","shell.execute_reply.started":"2023-05-17T12:50:28.832764Z","shell.execute_reply":"2023-05-17T12:50:29.179655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As shown above, there is no Nan in Prot_Peptides_df","metadata":{}},{"cell_type":"markdown","source":"## Select a subset of UniProt, which are shared in all visit_id samples","metadata":{}},{"cell_type":"code","source":"# group by visit_id and get the set of unique UniProt values for each group\ngrouped = Prot_Peptides_df.groupby('visit_id')['UniProt'].apply(set)\n\n# calculate the intersection of UniProt values across all groups\nintersection = set.intersection(*grouped)\n\n# print the intersection\nprint(intersection, len(intersection))","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:47:58.453781Z","iopub.execute_input":"2023-05-17T19:47:58.454127Z","iopub.status.idle":"2023-05-17T19:47:58.668994Z","shell.execute_reply.started":"2023-05-17T19:47:58.454101Z","shell.execute_reply":"2023-05-17T19:47:58.668071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Among all samples, there are 27 UniProt entries that are present.\n* Select the 27 UniProt as features and filter the Prot_Peptides_df to have the intersection. ","metadata":{}},{"cell_type":"code","source":"# set of UniProt values to select\nuniprots_to_select = intersection\n\n# filter the dataframe to select rows with UniProt values in the set\ndf_filtered = Prot_Peptides_df[Prot_Peptides_df['UniProt'].isin(uniprots_to_select)]\n# print the resulting dataframe\nprint(df_filtered)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:48:01.790023Z","iopub.execute_input":"2023-05-17T19:48:01.790410Z","iopub.status.idle":"2023-05-17T19:48:01.845608Z","shell.execute_reply.started":"2023-05-17T19:48:01.790378Z","shell.execute_reply":"2023-05-17T19:48:01.844483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing for 'PeptideAbundance'\n* Since the 'PeptideAbundance' column contains large integer features, we apply a logarithmic transformation (log(x+1)) to approximate normality and stabilize their variance. This transformation is commonly used to address skewness in data with a wide range of values. By applying this transformation, the values in these columns are shifted towards zero, resulting in a more symmetric distribution that is characteristic of normal distribution.\n\n* To ensure the transformed values have similar scales, we standardize the data using Z-score normalization on the log-transformed values of the original feature. This step ensures that each feature contributes equally to the analysis, regardless of their original scales.\n\n* Overall, applying log(x+1) to 'PeptideAbundance' columns and rescaling using Z-score normalization can improve the accuracy and reliability of subsequent analyses.\n","metadata":{}},{"cell_type":"code","source":"# apply log(x+1) transformation to columns\ndf_filtered[['PeptideAbundance']] = df_filtered[['PeptideAbundance']].apply(lambda x: np.log1p(x))\n\n# standardization\nscaler1 = StandardScaler()\ndf_filtered[['PeptideAbundance']] = scaler1.fit_transform(df_filtered[['PeptideAbundance']])\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:48:06.968923Z","iopub.execute_input":"2023-05-17T19:48:06.969267Z","iopub.status.idle":"2023-05-17T19:48:06.988267Z","shell.execute_reply.started":"2023-05-17T19:48:06.969224Z","shell.execute_reply":"2023-05-17T19:48:06.987277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a list of all peptides presented in the original data\nall_peptides = df_filtered['Peptide'].unique()\n# create a list of all visit_id in the original data\nall_visit_id = df_filtered['visit_id'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:48:10.326850Z","iopub.execute_input":"2023-05-17T19:48:10.327199Z","iopub.status.idle":"2023-05-17T19:48:10.389832Z","shell.execute_reply.started":"2023-05-17T19:48:10.327169Z","shell.execute_reply":"2023-05-17T19:48:10.388870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Protein-peptide network analysis\nProtein-peptide network analysis is a powerful tool for understanding the interactions between proteins and peptides in biological systems. It involves the creation of a network graph, where nodes represent proteins and peptides and edges represent their interactions or relationships.\nIn this project, we can create a network graph where each protein and peptide is represented as a node and their interactions are represented as edges. The weight of the edges are the PeptideAbundance. \n# Take one 'visit_id' for example to show how to construct a Protein-peptide Network.\n1. Construct a Prop-Peptide Network for the visit_id\n2. Protein-Peptide Network Visulization","metadata":{}},{"cell_type":"code","source":"visit_id ='55_12'\np_df = df_filtered[df_filtered['visit_id'] == visit_id]\n# first, pivot the data so that the UniProt column becomes the index, \n# the Peptide column becomes the columns, and the PeptideAbundance column becomes the values\npivoted = p_df.pivot(index='UniProt', columns='Peptide', values='PeptideAbundance')\n# iterate over each UniProt and add missing peptides as columns with a value of 0\nfor prot in pivoted.index:\n    missing_peptides = set(all_peptides) - set(pivoted.columns)\n    for pep in missing_peptides:\n        pivoted.loc[prot, pep] = 0\n\n# fill any remaining missing values with 0\npivoted.fillna(0, inplace=True)\n\n# Plot the heatmap\nsns.heatmap(pivoted)\n# Set the title of the heatmap\nplt.title(\"Peptide Abundance Matrix Heatmap\")\n\n# Show the heatmap\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:48:14.542911Z","iopub.execute_input":"2023-05-17T19:48:14.543222Z","iopub.status.idle":"2023-05-17T19:48:15.060994Z","shell.execute_reply.started":"2023-05-17T19:48:14.543197Z","shell.execute_reply":"2023-05-17T19:48:15.059922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature extractoin of UniProt-Peptides Network\n## Principal component analysis (PCA) is used to extract the most important components of the Protein-Peptides network. These components could then be used as features for downstream analysis\n\n# Average Protein-Peptides Networks among all visit_id","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components = 10)\n\n# initialize an empty list to store the PCA features\npca_features_list = []\n\nfor visit_id in all_visit_id:\n    # Part 1: construct UniProt-Peptide Network for each visit_id\n    p_df = df_filtered[df_filtered['visit_id'] == visit_id]\n    # first, pivot the data so that the UniProt column becomes the index, \n    # the Peptide column becomes the columns, and the PeptideAbundance column becomes the values\n    pivoted = p_df.pivot(index='UniProt', columns='Peptide', values='PeptideAbundance')\n    # iterate over each UniProt and add missing peptides as columns with a value of 0\n    for prot in pivoted.index:\n        missing_peptides = set(all_peptides) - set(pivoted.columns)\n        for pep in missing_peptides:\n            pivoted.loc[prot, pep] = 0\n\n    # fill any remaining missing values with 0\n    pivoted.fillna(0, inplace=True)\n    \n    # Part II: Sum up all networks of all visit_id \n    if visit_id == all_visit_id[0]:\n        avg_network = pivoted\n    else:\n        avg_network = avg_network.add(pivoted, fill_value=0)\n    \n    # Part III: PCA feature extraction\n    # reset the index so that UniProt is a column again\n    pivoted = pivoted.reset_index()\n    # rename the index column to 'UniProt'\n    pivoted.index.name = 'UniProt'\n    # save the pivoted DataFrame to a new variable called `new_df`\n    new_df = pivoted.copy(deep=True)\n    # fit and transform the data\n    df1_pca = pca.fit_transform(new_df.iloc[:,1:])\n    # flatten the array to one dimension\n    df1_pca = np.array(df1_pca).flatten()\n    # store the PCA features along with the visit_id in the list\n    pca_features_list.append({'visit_id': visit_id, 'pca_features': df1_pca})\n    \n# calculate the average network \navg_network = avg_network / len(all_visit_id)\n# construct a dataframe from the PCA features list\ndf_pca_features = pd.DataFrame(pca_features_list)\n# split the pca_features column into multiple columns\nPP_features = pd.concat([df_pca_features.drop('pca_features', axis=1), df_pca_features['pca_features'].apply(pd.Series)], axis=1)\n\n# display the resulting dataframe\nprint(PP_features)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:48:22.902493Z","iopub.execute_input":"2023-05-17T19:48:22.903377Z","iopub.status.idle":"2023-05-17T19:49:33.197124Z","shell.execute_reply.started":"2023-05-17T19:48:22.903327Z","shell.execute_reply":"2023-05-17T19:49:33.196023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Average Protein-Peptides Network Visulization","metadata":{}},{"cell_type":"code","source":"# Plot the heatmap\nsns.heatmap(avg_network)\n# Set the title of the heatmap\nplt.title(\"Average Peptide Abundance Matrix Heatmap\")\n# Show the heatmap\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:49:52.188686Z","iopub.execute_input":"2023-05-17T19:49:52.189764Z","iopub.status.idle":"2023-05-17T19:49:52.692062Z","shell.execute_reply.started":"2023-05-17T19:49:52.189710Z","shell.execute_reply":"2023-05-17T19:49:52.690907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## To create a training DataFrame for a deep learning model, we join the PP_features and ts_df_updrs DataFrames, and then remove any rows with NaN values.","metadata":{}},{"cell_type":"markdown","source":"# Part III: Deep learning model","metadata":{}},{"cell_type":"code","source":"\nPP_features1 = PP_features.merge(ts_df_updrs, on='visit_id')\n\ntarget_list = ['updrs_1_plus_0_months', 'updrs_2_plus_0_months',\n       'updrs_3_plus_0_months', 'updrs_4_plus_0_months',\n       'updrs_1_plus_6_months', 'updrs_2_plus_6_months',\n       'updrs_3_plus_6_months', 'updrs_4_plus_6_months',\n       'updrs_1_plus_12_months', 'updrs_2_plus_12_months',\n       'updrs_3_plus_12_months', 'updrs_4_plus_12_months',\n       'updrs_1_plus_24_months', 'updrs_2_plus_24_months',\n       'updrs_3_plus_24_months', 'updrs_4_plus_24_months']\ninfo_list =['visit_id','time', 'patient_id']\ncols_to_scale = PP_features1.columns.difference(target_list+info_list)\n\n#train_fea_target.columns[:-16]\ntrain_fea_target = PP_features1.copy(deep=True)\n# perform z-score normalization on the training data\nscaler_features = StandardScaler()\n# fit and transform the data\nscaler_features.fit(train_fea_target[cols_to_scale])\n#train_fea_target[cols_to_scale] = scaler.transform(train_fea_target[cols_to_scale])\ntrain_fea_target[cols_to_scale] = scaler_features.transform(train_fea_target[cols_to_scale])\n\n# Define the ratios for train, validation, and test sets\ntrain_ratio = 0.85\nval_ratio = 0.05\ntest_ratio = 0.10\n\n# Get a list of unique patient IDs in the DataFrame\npatient_ids = train_fea_target['patient_id'].unique()\n\n# Shuffle the list of patient IDs\nnp.random.shuffle(patient_ids)\n\n# Calculate the number of patients for each set\nnum_patients = len(patient_ids)\nnum_train_patients = int(num_patients * train_ratio)\nnum_val_patients = int(num_patients * val_ratio)\nnum_test_patients = num_patients - num_train_patients - num_val_patients\n\n# Split the patient IDs into train, validation, and test sets\ntrain_patients = patient_ids[:num_train_patients]\nval_patients = patient_ids[num_train_patients:num_train_patients+num_val_patients]\ntest_patients = patient_ids[num_train_patients+num_val_patients:]\n\n# Create train, validation, and test DataFrames using the patient ID splits\ntrain_df = train_fea_target[train_fea_target['patient_id'].isin(train_patients)].drop(columns= ['visit_id','time', 'patient_id'])\nval_df = train_fea_target[train_fea_target['patient_id'].isin(val_patients)].drop(columns= ['visit_id','time', 'patient_id'])\ntest_df = train_fea_target[train_fea_target['patient_id'].isin(test_patients)].drop(columns= ['visit_id','time', 'patient_id'])\n\n# Print the number of rows in each set\nprint(f'Training set: {len(train_df)} rows')\nprint(f'Validation set: {len(val_df)} rows')\nprint(f'Testing set: {len(test_df)} rows')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:49:59.821046Z","iopub.execute_input":"2023-05-17T19:49:59.821740Z","iopub.status.idle":"2023-05-17T19:49:59.870879Z","shell.execute_reply.started":"2023-05-17T19:49:59.821706Z","shell.execute_reply":"2023-05-17T19:49:59.869218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nX_train = train_df.iloc[:, :-16].astype(float)  # select all columns except the last 16\ny_train = train_df.iloc[:, -16:].astype(int)   # select only the last 16 columns\nX_val = val_df.iloc[:, :-16].astype(float)  # select all columns except the last 16\ny_val = val_df.iloc[:, -16:].astype(int)   # select only the last 16 columns\nX_test = test_df.iloc[:, :-16].astype(float)   # select all columns except the last 16\ny_test = test_df.iloc[:, -16:].astype(int)  # select only the last 16 columns\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:50:05.500504Z","iopub.execute_input":"2023-05-17T19:50:05.501065Z","iopub.status.idle":"2023-05-17T19:50:05.509617Z","shell.execute_reply.started":"2023-05-17T19:50:05.501020Z","shell.execute_reply":"2023-05-17T19:50:05.508463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smape_loss(y_true, y_pred):\n    y_true = K.cast(y_true,dtype='float32') + 1\n    y_pred = y_pred + 1\n    numer = K.abs(y_pred - y_true)\n    denom = K.abs(y_true) + K.abs(y_pred) \n    smape = 100 * numer / denom\n    return smape\n\n\ndef smape(y_true, y_pred):\n    \"\"\"\n    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE) between y_true and y_pred.\n    y_true -- array of true values\n    y_pred -- array of predicted values\n    \n    \"\"\"\n\n    y_true = y_true +1\n    y_pred = y_pred +1\n    numerator = np.abs(y_pred - y_true)\n    denominator = np.abs(y_pred) + np.abs(y_true) \n    \n    smape = 100 * np.mean(numerator / denominator)\n    return smape\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:50:08.814068Z","iopub.execute_input":"2023-05-17T19:50:08.814414Z","iopub.status.idle":"2023-05-17T19:50:08.821886Z","shell.execute_reply.started":"2023-05-17T19:50:08.814387Z","shell.execute_reply":"2023-05-17T19:50:08.820757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize variables to store the best model and its test score\n\nbest_model = None\nbest_test_score = float('inf')  # Set to infinity initially\n\nfor _ in range(5):\n    # Create a new model for each iteration\n    model = Sequential()\n    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n    model.add(Dropout(0.3))\n    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n    model.add(Dropout(0.3))\n    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n    model.add(Dropout(0.3))\n    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n    model.add(Dense(16, activation='linear'))\n\n    # Create Adam optimizer with a learning rate of 0.001\n    adam = Adam(learning_rate=0.0001)\n\n    # Compile model with Adam optimizer and a learning rate of 0.0001\n    model.compile(loss=smape_loss, optimizer='adam')\n    \n    # Fit the model\n    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, verbose=0, batch_size=32)\n    \n    # Evaluate the model on the test set\n    test_score = model.evaluate(X_test, y_test, verbose=0)\n    \n    # Check if the current model has a better test score than the previous best model\n    if test_score < best_test_score:\n        best_model = model\n        best_test_score = test_score\n    \n    print(test_score)\n    # Plot training and validation loss over epochs for each iteration\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model Loss')\n    plt.ylabel('MAPE')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\n# Use the best model for predictions\nmodel = best_model\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:57:46.278341Z","iopub.execute_input":"2023-05-17T19:57:46.278673Z","iopub.status.idle":"2023-05-17T19:57:56.306093Z","shell.execute_reply.started":"2023-05-17T19:57:46.278644Z","shell.execute_reply":"2023-05-17T19:57:56.305151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_test_score)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:58:51.331757Z","iopub.execute_input":"2023-05-17T19:58:51.332086Z","iopub.status.idle":"2023-05-17T19:58:51.336977Z","shell.execute_reply.started":"2023-05-17T19:58:51.332057Z","shell.execute_reply":"2023-05-17T19:58:51.335951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part IV: Submission ","metadata":{}},{"cell_type":"markdown","source":"Here, we create a new empty DataFrame new_df with the same structure as avg_network. We then fill in the values of new_df with values from df_filtered2 if available, and avg_network otherwise, using nested loops over the proteins and peptides. If a protein or peptide is not present in df_filtered2, we use the corresponding row or column from avg_network. Finally, we fill in any remaining missing values with 0.\n\nThe script iterates over the test data, applies preprocessing steps, including data filtering, scaling, and dimensionality reduction using PCA. The reduced features are then passed to the pre-trained model for prediction.","metadata":{}},{"cell_type":"code","source":"import amp_pd_peptide\nenv = amp_pd_peptide.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test files\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:59:09.641208Z","iopub.execute_input":"2023-05-17T19:59:09.642403Z","iopub.status.idle":"2023-05-17T19:59:09.662934Z","shell.execute_reply.started":"2023-05-17T19:59:09.642353Z","shell.execute_reply":"2023-05-17T19:59:09.661995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create a new DataFrame\nhist_df = pd.DataFrame(columns = ['patient_id']+ target_list)\n\nfor (test, test_peptides, test_proteins, sample_submission) in iter_test:\n    # Convert patient_id in test DataFrame to string\n    test['patient_id'] = test['patient_id'].astype(str)\n    \n    Prot_Peptides_df2 = pd.merge(test_proteins, test_peptides, on = ['visit_id','UniProt'], how = 'left')\n    Prot_Peptides_df2 = Prot_Peptides_df2[['visit_id','UniProt','Peptide','PeptideAbundance']]\n    df_filtered2 = Prot_Peptides_df2[Prot_Peptides_df2['UniProt'].isin(list(avg_network.index))]\n    \n    # apply log(x+1) transformation to columns\n    df_filtered2[['PeptideAbundance']] = df_filtered2[['PeptideAbundance']].apply(lambda x: np.log1p(x))\n    # standardization\n    df_filtered2[['PeptideAbundance']] = scaler1.fit_transform(df_filtered2[['PeptideAbundance']])\n\n    pca_features_list2 = []\n\n    for visit_id in df_filtered2['visit_id'].unique():\n        # Part 1: construct UniProt-Peptide Network for each visit_id\n        p_df = df_filtered2[df_filtered2['visit_id'] == visit_id]\n        # Create a new empty DataFrame using the structure of avg_network\n        new_df = pd.DataFrame(index=avg_network.index, columns=avg_network.columns)\n        # Fill in the new DataFrame with values from df_filtered2 or avg_network\n        for prot in avg_network.index:\n            if prot in p_df['UniProt'].values:\n                for peptide in avg_network.columns:\n                    if peptide in p_df[p_df['UniProt'] == prot]['Peptide'].values:\n                        new_df.loc[prot, peptide] = p_df[(p_df['UniProt'] == prot) & (p_df['Peptide'] == peptide)]['PeptideAbundance'].values[0]\n                    else:\n                        new_df.loc[prot, peptide] = 0 #avg_network.loc[prot, peptide]\n            else:\n                new_df.loc[prot, :] = avg_network.loc[prot, :]\n\n        # Fill any remaining missing values with 0\n        new_df.fillna(0, inplace=True)\n        \n        # Part II: PCA feature extraction\n        # fit and transform the data\n        df1_pca = pca.fit_transform(new_df) #.iloc[:,1:])\n        # flatten the array to one dimension\n        df1_pca = np.array(df1_pca).flatten()\n        # store the PCA features along with the visit_id in the list\n        pca_features_list2.append({'visit_id': visit_id, 'pca_features': df1_pca})\n\n    # construct a dataframe from the PCA features list\n    df_pca_features2 = pd.DataFrame(pca_features_list2)\n    # split the pca_features column into multiple columns\n    PP_features2 = pd.concat([df_pca_features2.drop('pca_features', axis=1), df_pca_features2['pca_features'].apply(pd.Series)], axis=1)\n    \n    col_fea=PP_features2.columns[1:]\n    X_test_sub = scaler_features.transform(PP_features2[col_fea])\n    # model.predict(tf.convert_to_tensor(X_test_sub)).astype(int)\n\n    test_result = np.maximum(model.predict(tf.convert_to_tensor(X_test_sub)),0) \n    # Create a new DataFrame with the visit_id and predicted target variables\n    result_df = pd.DataFrame({'visit_id': PP_features2['visit_id']})\n    for i in range(16):\n        result_df[target_list[i]]=test_result[:,i]\n            \n    # Create a new DataFrame to store the predictions\n    predictions = pd.DataFrame()\n    # Generate a prediction_id for each combination of visit_id and column name\n    for col in result_df.columns[1:]:\n        predictions = predictions.append(result_df[['visit_id', col]].rename(columns={col: 'rating'}).assign(prediction_id=lambda x: x['visit_id'] + '_' + col))\n        \n    for i, row in predictions.iterrows():\n        prediction_id = row['prediction_id']\n        rating = row['rating']\n        sample_submission.loc[sample_submission['prediction_id'] == prediction_id, 'rating'] = np.round(rating).astype(int)\n    \n    #####\n    # Add the new rows to the DataFrame\n    result_df['patient_id'] = result_df['visit_id'].str.split('_').str[0]\n    result_df = result_df.drop('visit_id', axis=1)\n\n    # Check if patient_id exists in hist_df\n    existing_ids = hist_df['patient_id'].unique()\n    new_ids = result_df[~result_df['patient_id'].isin(existing_ids)]\n\n    if not new_ids.empty:\n        hist_df = pd.concat([hist_df, new_ids], ignore_index=True)\n    else:\n        for idx, row in result_df.iterrows():\n            pid = row['patient_id']\n            hist_df.loc[hist_df['patient_id'] == pid, result_df.columns] = row.values\n        \n    # For the 'patient_id' values that exist in the DataFrame 'test' but not in the DataFrame 'test_proteins'.\n    patient_id_difference = set(test['patient_id'].unique()) - set(test_proteins['patient_id'].unique())\n    for patient_id in patient_id_difference:\n        if patient_id in hist_df['patient_id'].unique():\n            prediction_pat_id = patient_id +'_' + str(test.loc[test['patient_id']== patient_id,'visit_month'].values[-1])\n            for i in range(16):\n                prediction_id = prediction_pat_id  + '_' +target_list[i]\n                rating = hist_df.loc[hist_df['patient_id'] == patient_id].iloc[0, i+1]\n                sample_submission.loc[sample_submission['prediction_id'] == prediction_id, 'rating'] = int(rating)\n        \n        else:\n            # Create a dictionary with the values for the new row\n            new_row = {\n                'patient_id': patient_id,\n                'updrs_1_plus_0_months': 6,\n                'updrs_2_plus_0_months': 5,\n                'updrs_3_plus_0_months': 16,\n                'updrs_4_plus_0_months': 1,\n                'updrs_1_plus_6_months': 6,\n                'updrs_2_plus_6_months': 6,\n                'updrs_3_plus_6_months': 17,\n                'updrs_4_plus_6_months': 1,\n                'updrs_1_plus_12_months': 7,\n                'updrs_2_plus_12_months': 6,\n                'updrs_3_plus_12_months': 17,\n                'updrs_4_plus_12_months': 1,\n                'updrs_1_plus_24_months': 7,\n                'updrs_2_plus_24_months': 6,\n                'updrs_3_plus_24_months': 18,\n                'updrs_4_plus_24_months': 1\n            }\n            # Add the new row to the DataFrame\n            hist_df = hist_df.append(new_row, ignore_index=True)\n            prediction_pat_id = patient_id +'_' + str(test.loc[test['patient_id']== patient_id,'visit_month'].values[-1])\n            for i in range(16):\n                prediction_id = prediction_pat_id + '_' + target_list[i]\n                rating = hist_df.loc[hist_df['patient_id'] == patient_id].iloc[0, i+1]\n                sample_submission.loc[sample_submission['prediction_id'] == prediction_id, 'rating'] = int(rating)\n\n\n    env.predict(sample_submission[['prediction_id', 'rating']])   # register your predictions\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:59:12.145642Z","iopub.execute_input":"2023-05-17T19:59:12.145966Z","iopub.status.idle":"2023-05-17T19:59:22.805308Z","shell.execute_reply.started":"2023-05-17T19:59:12.145938Z","shell.execute_reply":"2023-05-17T19:59:22.804116Z"},"trusted":true},"execution_count":null,"outputs":[]}]}